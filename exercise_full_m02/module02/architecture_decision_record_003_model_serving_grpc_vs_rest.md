# ADR 003: 模型推理服务内部通信协议选择

*   **状态**: 已接受
*   **日期**: 2023-07-15
*   **决策者**: 架构委员会 (张工, 孙工, 李工)
*   **技术栈/上下文**: NovaBrain 3.0 核心后端服务，微服务架构，模型推理服务 (Inference Service) 需要与其他内部服务（如模型管理服务、API 网关）进行高频、低延迟通信。

## 问题陈述

我们需要为 NovaBrain 3.0 的模型推理服务与其调用者（主要是内部其他微服务）之间的通信选择一个标准协议。主要候选方案是 gRPC 和 RESTful API (基于 HTTP/1.1 或 HTTP/2，使用 JSON 或 Protobuf 作为载荷)。选择需要考虑性能（延迟、吞吐量）、开发效率、跨语言支持、服务发现、负载均衡、API 演进等因素。

## 考虑的方案

1.  **gRPC (基于 HTTP/2 + Protobuf)**
2.  **RESTful API (基于 HTTP/2 + Protobuf)**
3.  **RESTful API (基于 HTTP/1.1 + JSON)**

## 决策驱动因素

*   **性能 (低延迟、高吞吐量)**: 模型推理，特别是实时推理场景，对通信延迟非常敏感。协议本身的开销、序列化/反序列化效率是关键考量。
*   **开发效率与易用性**: 不同背景的开发团队（Python, Go, Java, Node.js）接入服务的便利性。
*   **强类型与 Schema 定义**: 需要明确的服务接口定义，便于团队协作和 API 演进。
*   **生态系统支持**: 服务发现、负载均衡、监控、追踪等基础设施的集成。
*   **流式处理**: 未来可能需要支持流式输入/输出的推理场景。

## 方案评估

| 因素             | 1. gRPC (HTTP/2+Proto) | 2. REST (HTTP/2+Proto) | 3. REST (HTTP/1.1+JSON) | 说明                                                                 |
| :--------------- | :--------------------- | :--------------------- | :---------------------- | :------------------------------------------------------------------- |
| **性能 (延迟)**  | ++ (二进制, 高效序列化) | + (二进制序列化)       | -- (文本, 序列化开销大)  | gRPC 基于 HTTP/2 长连接和头部压缩，Protobuf 效率高。                    |
| **性能 (吞吐量)**| ++ (多路复用)          | + (多路复用)           | - (连接开销大)           | HTTP/2 多路复用优势明显。                                            |
| **开发效率**     | +/- (需学习Proto, 工具链成熟) | - (需手动处理Proto)    | ++ (广泛熟悉, 简单)      | gRPC 工具链自动生成代码，但有学习曲线；JSON 最简单。                     |
| **跨语言支持**   | ++ (官方支持多种语言)   | +/- (Proto支持语言多, 但需手动集成) | ++ (几乎所有语言支持)      | gRPC 官方支持完善；REST+JSON 门槛最低。                            |
| **强类型/Schema**| ++ (Protobuf 强制)     | ++ (Protobuf 强制)     | - (JSON Schema 非强制) | Protobuf 提供强类型约束和接口定义。                                  |
| **生态系统**     | + (K8s 集成良好, 但部分网关支持弱) | ++ (标准HTTP, 网关/LB支持好) | ++ (同左)               | REST 更易与现有 HTTP 生态（LB, API Gateway）集成；gRPC 生态也在快速发展。 |
| **流式处理**     | ++ (原生支持双向流)    | - (需WebSocket或SSE等) | - (同左)                | gRPC 对流式场景支持最好。                                              |

## 决策

**选择方案 1: gRPC (基于 HTTP/2 + Protobuf)**

**理由**: 
1.  **性能优先**: 对于模型推理服务，特别是低延迟场景，gRPC 在协议开销、序列化效率和 HTTP/2 特性利用上具有显著优势。
2.  **强类型接口**: Protobuf 提供的 IDL (接口定义语言) 和强类型约束，非常适合需要明确服务契约的微服务架构，有利于团队协作和 API 长期维护。
3.  **开发效率可接受**: 虽然有 Protobuf 的学习曲线，但成熟的代码生成工具链可以显著提高开发效率，减少手写客户端/服务端代码的工作量。
4.  **流式处理潜力**: 原生支持流式处理，为未来扩展更复杂的 AI 应用场景（如流式语音识别、视频分析）奠定基础。
5.  **生态逐步成熟**: 虽然在某些传统 API 网关上的支持不如 REST，但云原生生态（如 Istio, Linkerd）对 gRPC 的支持越来越好，且内部服务间通信可以不依赖传统网关。

**潜在风险与缓解措施**: 
*   **学习曲线**: 为团队提供 Protobuf 和 gRPC 的培训和最佳实践文档。
*   **生态集成**: 对于需要通过传统 API 网关暴露的服务，考虑增加一个轻量级的 gRPC-Gateway 进行协议转换，或者选择性地为外部接口提供 RESTful API。
*   **调试**: gRPC 的二进制协议调试相对 JSON 不够直观，推广使用 Wireshark + Proto Dissector, grpcurl, gRPC UI 等调试工具。

## 后续步骤

*   制定 Protobuf 文件编写规范和版本管理策略。
*   在各后端语言项目中集成 gRPC 框架和代码生成工具。
*   提供 gRPC 服务开发和调试的最佳实践指南。
*   评估并引入 gRPC 服务发现和负载均衡方案（如 K8s Service, Istio）。 